#!/usr/bin/env python3

# sub_pipeline

import sys
import os
import platform
import pwd
import pathlib
import argparse
import json
import subprocess

from datetime import datetime


def retrieve_user_settings(pipeline, verbose=False):
    """ Load user's json and override defaults, but not cmdline """

    # Fill in missing arguments with user settings in home directory
    user_settings_file = os.path.join(
        os.path.expanduser("~"), ".pipelines.json"
    )
    if not os.path.isfile(user_settings_file):
        print("No settings found at {}.".format(user_settings_file))
        return {}

    user_settings = json.load(open(user_settings_file, "r"))
    if verbose:
        print("Found user settings at {}".format(user_settings_file))
    if pipeline not in user_settings:
        if verbose:
            print("  User settings have no pipeline '{}'".format(pipeline))
        return {}

    if verbose:
        print("  Pipeline '{}' found in user settings".format(pipeline))

    return user_settings[pipeline]


def get_arguments():
    """ Parse command line arguments """

    # Start with user settings as defaults
    user_settings = retrieve_user_settings(
        sys.argv[0], verbose=("--verbose" in sys.argv),
    )

    parser = argparse.ArgumentParser(
        description="Submit a Singularity pipeline request to SLURM.",
    )
    parser.add_argument(
        "pipeline",
        help="The name of a pipeline to execute",
    )
    parser.add_argument(
        "subject",
        help="The name of a subject to be processed",
    )
    parser.add_argument(
        "--home",
        help="Submitting user's home path",
    )
    parser.add_argument(
        "--userid",
        help="Submitting user's numeric user id",
    )
    parser.add_argument(
        "--username",
        help="Submitting user's user name",
    )
    parser.add_argument(
        "--rawdata",
        default=getattr(user_settings, "rawdata", None),
        help="The path to get bids-formatted raw data",
    )
    parser.add_argument(
        "--derivatives",
        default=getattr(user_settings, "derivatives", None),
        help="The path to write pipeline derivative output",
    )
    parser.add_argument(
        "--log-file",
        default=getattr(user_settings, "log-file", None),
        help="The path to a file for logging details",
    )
    parser.add_argument(
        "--work-directory",
        default=getattr(user_settings, "work-directory", None),
        help="The path to read and write temporary files",
    )
    parser.add_argument(
        "--staging-directory",
        default=getattr(user_settings, "staging-directory", None),
        help="The path to stage output before copying to derivatives",
    )
    parser.add_argument(
        "--templateflow-directory",
        default=getattr(user_settings, "templateflow-directory", None),
        help="The path to read and write cached templateflow data",
    )
    parser.add_argument(
        "--pipeline-version",
        default=getattr(user_settings, "pipeline-version", None),
        help="The version of the docker/singularity pipeline to run",
    )
    parser.add_argument(
        "--slurm-partition",
        default=getattr(user_settings, "slurm-partition", None),
        help="The slurm partition for this job",
    )
    parser.add_argument(
        "--freesurfer-license",
        default=getattr(user_settings, "freesurfer-license", None),
        help="The location of a valid FreeSurfer license file",
    )
    parser.add_argument(
        "--output-resolution",
        default=getattr(user_settings, "output-resolution", "2.0"),
        help="The isotropic resolution in mm for qsiprep output",
    )
    parser.add_argument(
        "--output-space",
        default=getattr(user_settings, "output-space", "T1w"),
        help="Anatomical space for qsiprep output, should stay with T1w.",
    )
    parser.add_argument(
        "--output-spaces",
        default=getattr(user_settings, "output-spaces", None),
        help="Anatomical space and resolution for fmiprep output",
    )
    parser.add_argument(
        "--template",
        default=getattr(user_settings, "template", "MNI152NLin2009cAsym"),
        help="The intended template for qsiprep",
    )
    parser.add_argument(
        "--recon-spec",
        default=getattr(user_settings, "recon-spec", None),
        help="The pipeline for diffusion, after pre-processing is complete",
    )
    parser.add_argument(
        "--hmc-model",
        default=getattr(user_settings, "hmc-model", "eddy"),
        help="The diffusion correction algorithm, 'eddy' or '3dSHORE'",
    )
    parser.add_argument(
        "--num-cpus",
        default=getattr(user_settings, "num-cpus", "4"),
        help="The number of processors to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-mb",
        default=getattr(user_settings, "memory-mb", "24576"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--stage-output", action="store_true",
        default=getattr(user_settings, "stage-output", False),
        help="Set this flag to cache output before copying to derivatives.",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        default=getattr(user_settings, "verbose", False),
        help="set to trigger verbose output",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        default=getattr(user_settings, "dry-run", False),
        help="set to create sbatch script, but NOT submit it.",
    )

    # Determine expected values for variables not provided or overridden
    if "--userid" not in sys.argv:
        sys.argv.append("--userid")
        sys.argv.append(str(os.getuid()))
    if "--username" not in sys.argv:
        sys.argv.append("--username")
        sys.argv.append(pwd.getpwuid(os.getuid())[0])
    if "--home" not in sys.argv:
        sys.argv.append("--home")
        sys.argv.append(os.path.expanduser("~"))

    args = parser.parse_args()
    setattr(args, "timestamp", datetime.now().strftime("%Y%m%d%H%M%S"))

    # Generate final defaults, if they aren't provided.
    if getattr(args, "work_directory") is None:
        setattr(args, "work_directory", os.path.join(
            "/tmp", "_".join([args.pipeline, args.timestamp, "working", ])
        ))
    if getattr(args, "staging_directory") is None:
        setattr(args, "staging_directory", os.path.join(
            "/tmp", "_".join([args.pipeline, args.timestamp, "staging", ])
        ))
    if getattr(args, "templateflow_directory") is None:
        setattr(args, "templateflow_directory", os.path.join(
            "/tmp", "templateflow"
        ))
    if getattr(args, "slurm_partition") is None:
        setattr(args, "slurm_partition", args.pipeline)

    return args


def get_singularity_image(args):
    """ Return the path to a singularity file, if it exists, else None """

    sif_path = "/var/lib/singularity/images"
    sif_file = None
    possibilities = []

    if args.pipeline_version is None:
        if args.verbose:
            print("No version specified for {} pipeline.".format(
                args.pipeline
            ))
        possibilities = list(pathlib.Path(sif_path).glob(
            "{}-*.sif".format(args.pipeline)
        ))
        if len(possibilities) <= 0:
            print("ERR: No image exists for {} pipeline on {}.".format(
                args.pipeline, platform.node()
            ))
        elif len(possibilities) == 1:
            sif_file = possibilities[0]
            if args.verbose:
                print("The only {} pipeline is version {}".format(
                    args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
                ))
        else:
            sif_file = sorted(possibilities, reverse=True)[0]
            print("Using latest {} pipeline, version {}".format(
                args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
            ))
    else:
        sif_file = pathlib.Path(os.path.join(
            sif_path, "{}-{}.sif".format(args.pipeline, args.pipeline_version)
        ))
        if not os.path.isfile(sif_file):
            print("ERR: No image exists at {}.".format(sif_file))
            sif_file = None

    return sif_file


def context_is_valid(args):
    """ Ensure paths are OK and writable before bothering slurm queues. """

    # Pipeline is available.
    setattr(args, "sif_file", get_singularity_image(args))
    sif_available = args.sif_file is not None

    # Subject is available in rawdata
    if args.rawdata is None:
        print("ERR: 'rawdata' must be provided in ~/.pipelines.json or as --rawdata.")
        sub_available = False
    elif args.subject is None:
        print("ERR: 'subject' must be provided on the command line.")
        sub_available = False
    else:
        sub_available = os.path.exists(
            os.path.join(args.rawdata, "sub-" + args.subject)
        )
        if not sub_available:
            print("ERR: Cannot find a file at '{}'".format(
                os.path.join(args.rawdata, "sub-" + args.subject)
            ))

    # Work directory is writable
    if args.work_directory is None:
        work_writable = False
    else:
        os.makedirs(args.work_directory, exist_ok=True)
        test_w_file = os.path.join(args.work_directory, "test.file")
        with open(test_w_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        work_writable = os.path.exists(test_w_file)
        os.remove(test_w_file)

    # Staging directory is writable
    if args.staging_directory is None:
        stage_writable = False
    else:
        os.makedirs(args.staging_directory, exist_ok=True)
        test_s_file = os.path.join(args.staging_directory, "test.file")
        with open(test_s_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        stage_writable = os.path.exists(test_s_file)
        os.remove(test_s_file)

    retval = (sif_available and sub_available and
              work_writable and stage_writable)
    print("Context is {}valid.".format("" if retval else "NOT "))
    return retval


def write_batch_script(args):
    """ Create the batch script to batch the pipeline execution. """

    os.makedirs(os.path.join(args.home, "bin", "slurm"), exist_ok=True)
    script_name = "_".join([
        "batch", "sub-" + args.subject, args.pipeline, args.timestamp,
    ]) + ".sbatch"
    batch_file_path = pathlib.Path(args.home, "bin", "slurm", script_name)
    if args.log_file is None:
        setattr(args, "log_file", str(batch_file_path).replace(
            ".sbatch", ".%j.out"
        ))
    with open(batch_file_path, "w") as batch_file:
        batch_file.write("#!/bin/bash\n")
        batch_file.write("#SBATCH --job-name={}\n".format(args.subject))
        batch_file.write("#SBATCH --partition={}\n".format(args.slurm_partition))
        batch_file.write("#SBATCH --output={}\n".format(args.log_file))
        batch_file.write("#SBATCH --error={}\n".format(args.log_file))
        batch_file.write("#SBATCH --time=0\n")
        batch_file.write("#SBATCH --ntasks-per-node={}\n".format(args.num_cpus))
        batch_file.write("#SBATCH --mem={}\n".format(args.memory_mb))
        batch_file.write("#SBATCH --chdir=/tmp/\n")
        batch_file.write("#SBATCH --export={}\n".format(
            "SINGULARITYENV_TEMPLATEFLOW_HOME=/opt/templateflow"
        ))
        batch_file.write("\n")
        batch_file.write("mkdir -p {}\n".format(args.work_directory))
        batch_file.write("mkdir -p {}\n".format(args.templateflow_directory))
        if args.stage_output:
            batch_file.write("mkdir -p {}\n".format(args.staging_directory))
        if args.verbose:
            batch_file.write("\n")
            batch_file.write("env\n")
        batch_file.write("\n")
        batch_file.write("singularity run --cleanenv \\\n")
        batch_file.write("  -B {}:/rawdata:ro \\\n".format(args.rawdata))
        if args.stage_output:
            batch_file.write("  -B {}:/out \\\n".format(args.staging_directory))
        else:
            batch_file.write("  -B {}:/out \\\n".format(args.derivatives))
        batch_file.write("  -B {}:/work \\\n".format(args.work_directory))
        batch_file.write("  -B {}:/opt/templateflow \\\n".format(
            args.templateflow_directory
        ))
        batch_file.write("  -B {}:/opt/freesurfer/license.txt:ro \\\n".format(
            args.freesurfer_license
        ))
        batch_file.write("  {} \\\n".format(args.sif_file))
        batch_file.write("  /rawdata /out participant \\\n")
        batch_file.write("  -w /work \\\n")
        batch_file.write("  --fs-license-file /opt/freesurfer/license.txt \\\n")
        batch_file.write("  --participant-label {} \\\n".format(args.subject))
        if args.pipeline == "fmriprep":
            if args.output_spaces is not None:
                batch_file.write("  --output-spaces {} \\\n".format(
                    args.output_spaces
                ))
        elif args.pipeline == "qsiprep":
            batch_file.write("  --output-resolution {} \\\n".format(
                args.output_resolution
            ))
            batch_file.write("  --hmc-model {} \\\n".format(args.hmc_model))
            batch_file.write("  --output-space {} \\\n".format(args.output_space))
            batch_file.write("  --template {} \\\n".format(args.template))
            if (args.recon_spec is not None) and (args.recon_spec != "none"):
                batch_file.write("  --recon-spec {} \\\n".format(args.recon_spec))
        batch_file.write("  --nthreads {} --mem-mb {}\n".format(
            int(args.num_cpus) * 2, args.memory_mb
        ))
        batch_file.write("\n")
        batch_file.write("if [[ \"$?\" != \"0\" ]]; then\n")
        batch_file.write("  echo \"Container run failed. Avoiding clean-up.\"\n")
        batch_file.write("  echo \"$(hostname):{}\"\n".format(args.work_directory))
        if args.stage_output:
            batch_file.write("  echo \"$(hostname):{}\"\n".format(args.staging_directory))
        else:
            batch_file.write("  echo \"{}\"\n".format(args.derivatives))
        batch_file.write("  exit $?\n")
        batch_file.write("fi\n")
        batch_file.write("\n")
        batch_file.write("# Format and copy output to final destination.\n")
        if args.stage_output:
            batch_file.write("chown {}:{} --recursive {}\n".format(
                args.username, args.userid, args.staging_directory
            ))
            batch_file.write("rsync -ah {}/{}/sub-{}* {}/{}/\n".format(
                args.staging_directory, args.pipeline, args.subject,
                args.derivatives, args.pipeline
            ))
            if args.pipeline == "fmriprep":
                batch_file.write("if [[ ! -e {}/freesurfer/sub-{} ]]; then\n".format(
                    args.derivatives, args.subject,
                ))
                batch_file.write("  rsync -ah {}/freesurfer/sub-{}* {}/freesurfer/\n".format(
                    args.staging_directory, args.subject, args.derivatives,
                ))
                batch_file.write("else\n")
                batch_file.write("  echo \"Freesurfer output at {} was abandoned\"\n".format(
                    args.staging_directory,
                ))
                batch_file.write("  echo \"to avoid over-writing {}/freesurfer/sub-{}\"\n".format(
                    args.derivatives, args.subject,
                ))
                batch_file.write("fi\n")
            elif args.pipeline == "qsiprep":
                batch_file.write("rsync -ah {}/qsirecon/sub-{}* {}/qsirecon/\n".format(
                    args.staging_directory, args.subject, args.derivatives,
                ))
        batch_file.write("rm -rf {}\n".format(args.work_directory))
        batch_file.write("# intentionally leaving templateflow dir\n")
        batch_file.write("\n")
        batch_file.write("echo \"Job complete at $(date)\"\n")
        batch_file.write("\n")
        batch_file.write("# rm -rf {}\n".format(args.staging_directory))

    return batch_file_path


def submit_batch_script(sbatch_file, dry_run=False, verbose=False):
    """ Create the batch script to batch the pipeline execution. """
    if verbose:
        print("|==========---------- Batch file follows ----------==========|")
        subprocess.run(["cat", sbatch_file, ], check=True)
        print("|==========----------        end         ----------==========|")
    print("Batch file was saved to {}".format(sbatch_file))
    if dry_run:
        print("NOT submitting job because dry-run is True.")
    else:
        print("Submitting job at {}".format(
            datetime.now().strftime("%Y%m%d %H:%M:%S")
        ))
        sbatch_submit_process = subprocess.run(
            ["sbatch", sbatch_file, ], check=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
        )
        print(sbatch_submit_process.stdout.decode("utf-8"))


def main(args):
    """ Entry point """

    if args.verbose:
        print("Asked to process data from {} with options:".format(
            args.subject
        ))
        for arg in vars(args):
            print("  {}: {}".format(arg, getattr(args, arg)))

    if context_is_valid(args):
        batch_script = write_batch_script(args)
        submit_batch_script(
            batch_script, dry_run=args.dry_run, verbose=args.verbose
        )


if __name__ == "__main__":
    main(get_arguments())
