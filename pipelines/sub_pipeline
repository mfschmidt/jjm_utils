#!/usr/bin/env python3

# sub_pipeline

import sys
import os
import platform
import pwd
from pathlib import Path
import json
import subprocess

import argparse
from argparse import RawTextHelpFormatter

from datetime import datetime


class SlurmBatch():
    """ Represents a SLURM sbatch file """

    def __init__(self, args):
        """ Store args into object and start to interpret them. """
        self.args = args

        # Create the path and figure out the file names.
        self.path = Path(self.args.home) / "bin" / "slurm"
        self.filename = "_".join([
            "batch", "sub-" + args.subject, args.pipeline, args.timestamp,
        ]) + ".sbatch"

        if self.args.log_file is None:
            setattr(args, "log_file",
                    self.filename.replace(".sbatch", ".%j.out"))

    def header(self):
        """ Return the header with SLURM variables to the sbatch file.
        """
        tf_env = "SINGULARITYENV_TEMPLATEFLOW_HOME"
        mpl_env = "SINGULARITYENV_MPLCONFIGDIR"
        return "\n".join([
            "#!/bin/bash",
            "",
            f"#SBATCH --job-name={self.args.subject}",
            f"#SBATCH --partition={self.args.slurm_partition}",
            f"#SBATCH --output={self.args.log_file}",
            f"#SBATCH --error={self.args.log_file}",
            "#SBATCH --time=0",
            f"#SBATCH --ntasks-per-node={self.args.num_threads}",
            f"#SBATCH --mem={self.args.memory_mb}",
            f"#SBATCH --chdir={self.args.tmp_root}",
            "",
            f"export {tf_env}=/opt/templateflow",
            f"export {mpl_env}=/opt/matplotlib",
            "",
            f"mkdir -p {self.args.work_directory}",
            f"mkdir -p {self.args.templateflow_directory}",
            f"mkdir -p {self.args.matplotlib_directory}",
            f"mkdir -p {self.args.staging_directory}",
            "",
            "",
        ])

    def singularity_mounts(self):
        """ Return the mounts necessary for running the pipeline
        """

        return [
            {"host_path": "/run/shm",
             "cont_path": "/run/shm",
             "permissions": "rw", },
            {"host_path": "/data",
             "cont_path": "/data",
             "permissions": "ro", },
            {"host_path": self.args.rawdata,
             "cont_path": "/rawdata",
             "permissions": "ro", },
            {"host_path": self.args.staging_directory,
             "cont_path": "/out",
             "permissions": "rw", },
        ]

    def singularity_command(self):
        """ Return the singularity command to execute in the sbatch file.

            Some of these mounts don't matter for all situations, but mounting
            them doesn't hurt either, so they all stay.
        """
        base_command = [
            "singularity run --cleanenv",
        ] + [
            f"  -B {m['host_path']}:{m['cont_path']}:{m['permissions']}"
            for m in self.singularity_mounts()
        ] + [
            f"  {self.args.sif_file}",
        ]
        return " \\\n".join(base_command)

    def failure_text(self, what_failed):
        """ Return the text to report a failure in the sbatch file.
        """

        failure_lines = [
            "\nif [[ \"$?\" != \"0\" ]]; then",
            f"  echo \"{what_failed} failed.\"",
            f"  echo \"$(hostname):{self.args.work_directory}\"",
            f"  echo \"$(hostname):{self.args.staging_directory}\"",
            "  exit $?",
            "fi\n",
        ]
        return "\n".join(failure_lines) + "\n"

    def singularity_arguments(self):
        """ To be overridden by each specific type of pipeline.
        """

        return ""

    def singularity_addons(self):
        """ To be overridden by each specific type of pipeline.
        """

        return ""

    def move_staged_output_commands(self):
        """ If output is staged locally, supply commands to relocate it.
        """

        return "\n".join([
            "# Format and copy output to final destination.",
            "chown {}:{} --recursive {}".format(
                self.args.username, self.args.userid,
                self.args.staging_directory
            ),
            "rsync -ah {}/{}/sub-{}* {}/{}/".format(
                self.args.staging_directory, self.args.pipeline,
                self.args.subject,
                self.args.derivatives, self.args.pipeline
            ),
        ])

    def cleanup_steps(self):
        """ To be overridden by each specific type of pipeline.
        """

        return ""

    def write(self):
        """ Create the batch script to batch the pipeline execution. """

        # Create the path and figure out the file names.
        self.path.mkdir(exist_ok=True)
        batch_file_path = self.path / self.filename

        # Open and write the sbatch file.
        with open(batch_file_path, "w") as batch_file:
            batch_file.write(self.header())
            batch_file.write(self.singularity_command() + " \\\n")
            # TODO: Somewhere in here, pass along any arguments we did not
            #       explicitly handle.
            batch_file.write(self.singularity_arguments())
            batch_file.write("\n")
            batch_file.write(self.failure_text("Container"))

            # FreeSurfer does additional segmentations as separate processes
            batch_file.write(self.singularity_addons())

            # If we are staging output, handle moving it to its final home
            if self.args.stage_output:
                batch_file.write(
                    self.move_staged_output_commands()
                )

            batch_file.write(self.cleanup_steps())

            batch_file.write("\n".join([
                "echo \"Job complete at $(date)\"",
                f"rm -rf {self.args.work_directory}",
                "# leave the templateflow directory for next time.",
                ""
            ]))
            if self.args.stage_output:
                batch_file.write(
                    f"rm -rf {self.args.staging_directory}\n"
                )
        return batch_file_path

    def submit(self):
        """ Submit the created file. """
        if self.args.verbose:
            print("|=========--------- Batch file follows ---------=========|")
            subprocess.run(["cat", str(self.path / self.filename), ],
                           check=True)
            print("|=========---------   end batch file   ---------=========|")
        print(f"Batch file was saved to {str(self.path / self.filename)}")
        if self.args.dry_run:
            print("NOT submitting job because dry-run is True.")
        else:
            print("Submitting job at {}".format(
                datetime.now().strftime("%Y%m%d %H:%M:%S")
            ))
            sbatch_submit_process = subprocess.run(
                ["sbatch", str(self.path / self.filename), ], check=True,
                stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
            )
            print(sbatch_submit_process.stdout.decode("utf-8"))


class FreeSurferBatch(SlurmBatch):
    """ Extend SlurmBatch to handle FreeSurfer """

    def __init__(self, args):
        super().__init__(args)
        self.pipeline = "freesurfer"
        self.subject_path = (
            Path(self.args.rawdata) / f"sub-{self.args.subject}"
        )

    def singularity_mounts(self):
        """ Return the mounts necessary for running the pipeline
        """

        return super().singularity_mounts() + [
            {"host_path": self.args.work_directory,
             "cont_path": "/work/",
             "permissions": "rw", },
            {"host_path": self.args.templateflow_directory,
             "cont_path": "/opt/templateflow",
             "permissions": "rw", },
            {"host_path": self.args.matplotlib_directory,
             "cont_path": "/opt/matplotlib",
             "permissions": "rw", },
            {"host_path": self.args.freesurfer_license,
             "cont_path": "/opt/freesurfer/license.txt",
             "permissions": "ro", },
        ]

    def singularity_arguments(self):
        """ supply additional freesurfer-specific arguments """

        # Use specified T1 & T2 files, or find them in BIDS-compatible rawdata
        if self.args.input_t1 == "":
            t1s = [str(p) for p in self.subject_path.glob("**/*T1w.nii.gz")]
        else:
            t1s = [self.args.input_t1, ]
        if self.args.input_t2 == "":
            t2s = [str(p) for p in self.subject_path.glob("**/*T2w.nii.gz")]
        else:
            t2s = [self.args.input_t2, ]

        # Use -cm if sub-1mm voxels are desired.
        cm_str = " -cm" if self.args.cm else ""

        further_arguments = [
            f"  recon-all -s sub-{self.args.subject}",
        ] + [
            f"  -i {t1.replace(self.args.rawdata, '/rawdata')}" for t1 in t1s
        ] + [
            f"  -T2 {t2.replace(self.args.rawdata, '/rawdata')}" for t2 in t2s
        ] + [
            f"  -sd /out -openmp {self.args.num_cpus}{cm_str} -all",
        ]

        return " \\\n".join(further_arguments)

    def addon_arguments(self, stage):
        """ supply additional freesurfer-specific arguments """

        # Use specified T1 & T2 files, or find them in BIDS-compatible rawdata
        if self.args.input_t2 == "":
            t2 = [str(p) for p in self.subject_path.glob("**/*T2w.nii.gz")][0]
        else:
            t2 = self.args.input_t2

        if stage == "hippocampus_t1":
            further_arguments = [
                f"  segmentHA_T1.sh sub-{self.args.subject} /out",
            ]
        elif stage == "hippocampus_t2":
            further_arguments = [
                f"  segmentHA_T2.sh sub-{self.args.subject} {t2} T2 1 /out",
            ]
        elif stage == "thalamus_t1":
            further_arguments = [
                f"  segmentThalamicNuclei.sh sub-{self.args.subject} /out",
            ]
        elif stage == "thalamus_t2":
            further_arguments = [
                f"  segmentThalamicNuclei.sh sub-{self.args.subject} {t2}"
                " T2 1 /out",
            ]
        elif stage == "brainstem":
            further_arguments = [
                f"  segmentBS.sh sub-{self.args.subject} /out",
            ]
        return " \\\n".join(further_arguments)

    def singularity_addons(self):
        """ Return the text of additional processes for FreeSurfer 7 runs.
        """

        addons = [
            self.singularity_command() + " \\",
            self.addon_arguments("hippocampus_t1"),
            self.failure_text("Hippocampal Subfields 1st Segmentation"),
        ]
        addons += [
            self.singularity_command() + " \\",
            self.addon_arguments("brainstem"),
            self.failure_text("Brainstem Nuclei Segmentation"),
        ]
        addons += [
            self.singularity_command() + " \\",
            self.addon_arguments("thalamus_t1"),
            self.failure_text("Thalamus 1st Segmentation"),
        ]
        if self.args.input_t2 == "":
            t2s = [str(p) for p in self.subject_path.glob("**/*T2w.nii.gz")]
        else:
            t2s = [self.args.input_t2, ]
        if len(t2s) > 0:
            addons += [
                self.singularity_command() + " \\",
                self.addon_arguments("hippocampus_t2"),
                self.failure_text("Hippocampal Subfields 2nd Segmentation"),
            ]
            addons += [
                self.singularity_command() + " \\",
                self.addon_arguments("thalamus_t2"),
                self.failure_text("Thalamus 2nd Segmentation"),
            ]
        return "\n".join(addons)

    def move_staged_output_commands(self):
        """ If output is staged locally, supply commands to relocate it.
        """

        return "\n".join([
            super().move_staged_output_commands(),
            "rsync -ah {}/sub-{} {}/".format(
                self.args.staging_directory,
                self.args.subject,
                self.args.subjects_dir,
            ),
            "",
        ])


class QSIPrepBatch(SlurmBatch):
    """ Extend SlurmBatch to handle QSIPrep """

    def __init__(self, args):
        super().__init__(args)
        self.pipeline = "qsiprep"

    def singularity_arguments(self):
        """ Return the arguments necessary for fmriprep, specifically.
        """
        further_arguments = [
            "  /rawdata /out participant",
            f"  --participant-label {self.args.subject} -w /work/",
            "  --fs-license-file /opt/freesurfer/license.txt",
            f"  --output-resolution {self.args.output_resolution}",
            f"  --hmc-model {self.args.hmc_model}",
            f"  --output-space {self.args.output_space}",
            f"  --template {self.args.template}",
            f"  --nthreads {self.args.num_threads}",
            f"  --mem_mb {self.args.memory_mb}",
        ]
        if (self.args.recon_spec is not None) \
           and (self.args.recon_spec != "none"):
            further_arguments.append(
                f"  --recon-spec {self.args.recon_spec}"
            )
        return " \\\n".join(further_arguments)

    def move_staged_output_commands(self):
        """ If output is staged locally, supply commands to relocate it.
        """

        return "\n".join([
            super().staged_output_commands(),
            "rsync -ah {}/qsirecon/sub-{}* {}/qsirecon/".format(
                self.args.staging_directory,
                self.args.subject,
                self.args.derivatives,
            ),
            ""
        ])


class FMRIPrepBatch(SlurmBatch):
    """ Extend SlurmBatch to handle FMRIPrep """

    def __init__(self, args):
        super().__init__(args)
        self.pipeline = "fmriprep"

    def get_fmriprep_singularity_arguments(self):
        """ Return the arguments necessary for fmriprep, specifically
        """
        further_arguments = [
            "  /rawdata /out participant",
            f"  --participant-label {self.args.subject} -w /work/",
            "  --fs-license-file /opt/freesurfer/license.txt",
        ]
        if self.args.pipeline_version.startswith("1."):
            if self.args.output_space is not None:
                further_arguments += [
                    f"  --nthreads {self.args.num_threads}",
                    f"  --mem_mb {self.args.memory_mb}",
                    f"  --output-space {self.args.output_space}",
                    f"  --template {self.args.template}",
                    "  --template-resampling-grid " +
                    f"{self.args.template_resampling_grid}",
                ]
        else:
            if self.args.output_spaces is not None:
                further_arguments += [
                    f"  --nprocs {self.args.num_threads}",
                    f"  --mem {self.args.memory_gb}",
                    f"  --output-spaces {self.args.output_spaces}",
                ]
        return " \\\n".join(further_arguments)

    def move_staged_output_commands(self):
        """ If output is staged locally, supply commands to relocate it.
        """

        sfs_dir = f"{self.args.derivatives}/freesurfer{self.args.version[0]}"
        return "\n".join([
            super().move_staged_output_commands(),
            "# Find an open location for freesurfer output.",
            f"F={sfs_dir}/sub-{self.args.subject}",
            "if [[ -e ${{F}} ]]; then",
            "  N=0",
            "  while [[ -e ${{F}} ]]; do",
            "    echo \"will not overwrite existing data, ${{F}}\"",
            "    N=$(( N + 1 ))",
            "    F=${{F%%.*}}.${{N}}",
            "  done",
            "fi",
            f"rsync -ah ${{F}}* {sfs_dir}/",
            ""
        ])


class MRIQCBatch(SlurmBatch):
    """ Extend SlurmBatch to handle MRIQC
    """

    def __init__(self, args):
        super().__init__(args)
        self.pipeline = "mriqc"

    def singularity_arguments(self):
        """ Return the arguments necessary for mriqc, specifically
        """
        return " \\\n".join([
            "  /rawdata /out participant",
            f"  --participant-label {self.args.subject}",
            "  -w /work",
            f"  --nprocs {self.args.num_threads}",
            "  --omp-nthreads 8",
            f"  --mem_gb {self.args.memory_gb}",
        ])

    def cleanup_steps(self):
        """ To be overridden by each specific type of pipeline.
        """

        return "\n".join([
            "",
            "# Move reports into subject's sub-directory.",
            f"mv {self.args.derivatives}/sub-{self.args.subject}_*.html "
            f"{self.args.derivatives}/sub-{self.args.subject}/",
            "",
            "",
        ])


def retrieve_user_settings(pipeline, verbose=False):
    """ Load user's json and override defaults, but not cmdline """

    # Fill in missing arguments with user settings in home directory
    user_settings_file = os.path.join(
        os.path.expanduser("~"), ".pipelines.json"
    )
    if not os.path.isfile(user_settings_file):
        print("No settings found at {}.".format(user_settings_file))
        return {}

    user_settings = json.load(open(user_settings_file, "r"))
    if verbose:
        print("Found user settings at {}".format(user_settings_file))
    if pipeline not in user_settings:
        if verbose:
            print("  User settings have no pipeline '{}'".format(pipeline))
        return {}

    if verbose:
        print("  Pipeline '{}' found in user settings".format(pipeline))

    return user_settings[pipeline]


def settle_memory(args):
    """ Convert memory specs.
        Different pipelines want different keys, make all available.
    """
    if args.memory_mb is None and args.memory_gb is None:
        # Default memory settings
        # FreeSurfer docs request 8GB, this should be overkill.
        # fMRIPrep docs request 8GB, this should be overkill.
        setattr(args, "memory_mb", 16 * 1024)
        setattr(args, "memory_gb", 16)
    elif args.memory_gb is None:
        setattr(args, "memory_mb", int(args.memory_mb))
        setattr(args, "memory_gb", int(args.memory_mb / 1024))
    elif args.memory_mb is None:
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_gb * 1024))
    else:
        # Both are set. -- unlikely, but fine
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_mb))


def settle_threads(args):
    """ Convert threads == cpus
        We only use num_threads, but we'll accept num_cpus as the same thing.
    """
    if args.num_cpus is None and args.num_threads is None:
        setattr(args, "num_cpus", 8)
        setattr(args, "num_threads", 8)
    elif args.num_threads is None:
        setattr(args, "num_cpus", int(args.num_cpus))
        setattr(args, "num_threads", int(args.num_cpus))
    elif args.num_cpus is None:
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_threads))
    else:
        # Both are set, unlikely, but just use what's given.
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_cpus))


def settle_subject(args):
    """ Allow "U00000" or "sub-U00000" as the same thing. """
    if args.subject.startswith("sub-"):
        setattr(args, "subject", args.subject[4:])


def get_arguments():
    """ Parse command line arguments """

    # Start with user settings as defaults
    user_settings = retrieve_user_settings(
        sys.argv[1], verbose=("--verbose" in sys.argv),
    )

    parser = argparse.ArgumentParser(
        description="\n".join([
            "Submit a Singularity pipeline request to SLURM.",
            "",
            "Save consistent settings to ~/.pipelines.json.",
            "Use command-line to override or supply infrequent options.",
            "See README at https://github.com/mfschmidt/jjm_utils/",
        ]),
        formatter_class=RawTextHelpFormatter,
    )
    parser.add_argument(
        "pipeline",
        help="The name of a pipeline to execute",
    )
    parser.add_argument(
        "subject",
        help="The name of a subject to be processed",
    )
    parser.add_argument(
        "--home",
        help="Submitting user's home path",
    )
    parser.add_argument(
        "--userid",
        help="Submitting user's numeric user id",
    )
    parser.add_argument(
        "--username",
        help="Submitting user's user name",
    )
    parser.add_argument(
        "--rawdata",
        default=user_settings.get("rawdata"),
        help="The path to get bids-formatted raw data",
    )
    parser.add_argument(
        "--derivatives",
        default=user_settings.get("derivatives"),
        help="The path to write pipeline derivative output",
    )
    parser.add_argument(
        "--subjects-dir",
        default=user_settings.get("subjects-dir"),
        help="The path to write freesurfer output",
    )
    parser.add_argument(
        "--log-file",
        default=user_settings.get("log-file"),
        help="The path to a file for logging details",
    )
    parser.add_argument(
        "--work-directory",
        default=user_settings.get("work-directory"),
        help="The path to read and write temporary files",
    )
    parser.add_argument(
        "--staging-directory",
        default=user_settings.get("staging-directory"),
        help="The path to stage output before copying to derivatives",
    )
    parser.add_argument(
        "--templateflow-directory",
        default=user_settings.get("templateflow-directory"),
        help="The path to read and write cached templateflow data",
    )
    parser.add_argument(
        "--matplotlib-directory",
        default=user_settings.get("matplotlib-directory"),
        help="The path to read and write cached matplotlib data",
    )
    parser.add_argument(
        "--pipeline-version",
        default=user_settings.get("pipeline-version"),
        help="The version of the docker/singularity pipeline to run",
    )
    parser.add_argument(
        "--slurm-partition",
        default=user_settings.get("slurm-partition"),
        help="The slurm partition for this job",
    )
    parser.add_argument(
        "--freesurfer-license",
        default=user_settings.get("freesurfer-license"),
        help="The location of a valid FreeSurfer license file",
    )
    parser.add_argument(
        "--output-resolution",
        default=user_settings.get("output-resolution", "2.0"),
        help="The isotropic resolution in mm for qsiprep output",
    )
    parser.add_argument(
        "--template-resampling-grid",
        default=user_settings.get("template-resampling-grid", "2mm"),
        help="The isotropic resolution for old fmriprep output",
    )
    parser.add_argument(
        "--output-space",
        default=user_settings.get("output-space", "T1w"),
        help="Anatomical space for qsiprep and old fmriprep output,"
             " should stay with T1w.",
    )
    parser.add_argument(
        "--output-spaces",
        default=user_settings.get("output-spaces"),
        help="Anatomical space and resolution for new fmiprep output",
    )
    parser.add_argument(
        "--template",
        default=user_settings.get("template", "MNI152NLin2009cAsym"),
        help="The intended template for qsiprep and old fmriprep",
    )
    parser.add_argument(
        "--recon-spec",
        default=user_settings.get("recon-spec"),
        help="The pipeline for diffusion, after pre-processing is complete",
    )
    parser.add_argument(
        "--hmc-model",
        default=user_settings.get("hmc-model", "eddy"),
        help="The diffusion correction algorithm, 'eddy' or '3dSHORE'",
    )
    parser.add_argument(
        "--input-t1",
        default=user_settings.get("input-t1", ""),
        help="Specify a specific T1w image for FreeSurfer input.",
    )
    parser.add_argument(
        "--input-t2",
        default=user_settings.get("input-t2", ""),
        help="Specify a specific T2w image for FreeSurfer input.",
    )
    parser.add_argument(
        "--num-cpus",
        default=user_settings.get("num-cpus"),
        help="The number of cpus==threads to allocate for the pipeline.",
    )
    parser.add_argument(
        "--num-threads",
        default=user_settings.get("num-threads"),
        help="The number of threads==cpus to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-mb",
        default=user_settings.get("memory-mb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-gb",
        default=user_settings.get("memory-gb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--cm", action="store_true",
        default=user_settings.get("cm", False),
        help="Set this flag to use sub-mm voxels in FreeSurfer.",
    )
    parser.add_argument(
        "--stage-output", action="store_true",
        default=user_settings.get("stage-output", False),
        help="Set this flag to cache output before copying to derivatives.",
    )
    parser.add_argument(
        "--tmp-root",
        default=user_settings.get("tmp-root", "/var/tmp"),
        help="Set this path to house default working and staging areas.",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        default=user_settings.get("verbose", False),
        help="set to trigger verbose output",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        default=user_settings.get("dry-run", False),
        help="set to create sbatch script, but NOT submit it.",
    )

    # Determine expected values for variables not provided or overridden
    if "--userid" not in sys.argv:
        sys.argv.append("--userid")
        sys.argv.append(str(os.getuid()))
    if "--username" not in sys.argv:
        sys.argv.append("--username")
        sys.argv.append(pwd.getpwuid(os.getuid())[0])
    if "--home" not in sys.argv:
        sys.argv.append("--home")
        sys.argv.append(os.path.expanduser("~"))

    args = parser.parse_args()
    setattr(args, "timestamp", datetime.now().strftime("%Y%m%d%H%M%S"))

    settle_memory(args)
    settle_threads(args)
    settle_subject(args)

    # Generate final defaults, if they aren't provided.
    if getattr(args, "work_directory") is None:
        setattr(args, "work_directory", os.path.join(
            args.tmp_root,
            "_".join([
                args.pipeline, args.subject, args.timestamp, "working",
            ])
        ))
    if getattr(args, "templateflow_directory") is None:
        setattr(args, "templateflow_directory", os.path.join(
            args.tmp_root, "templateflow"
        ))
    if getattr(args, "matplotlib_directory") is None:
        setattr(args, "matplotlib_directory", os.path.join(
            args.tmp_root, "matplotlib"
        ))
    if getattr(args, "slurm_partition") is None:
        setattr(args, "slurm_partition", args.pipeline)

    # If staging output, set up the directory, otherwise just copy derivatives
    if args.stage_output:
        if getattr(args, "staging_directory") is None:
            setattr(args, "staging_directory", os.path.join(
                args.tmp_root,
                "_".join([
                    args.pipeline, args.subject, args.timestamp, "staging",
                ])
            ))
    else:
        # If we are writing output directly, set staging_directory there.
        if args.pipeline == "freesurfer":
            setattr(args, "staging_directory", args.subjects_dir)
        else:
            setattr(args, "staging_directory", args.derivatives)

    return args


def get_singularity_image(args):
    """ Return the path to a singularity file, if it exists, else None """

    sif_path = "/var/lib/singularity/images"
    sif_file = None
    possibilities = []

    if args.pipeline_version is None:
        if args.verbose:
            print("No version specified for {} pipeline.".format(
                args.pipeline
            ))
        possibilities = [str(p) for p in Path(sif_path).glob(
            "{}-*.sif".format(args.pipeline)
        )]
        if len(possibilities) <= 0:
            print("ERR: No image exists for {} pipeline on {}.".format(
                args.pipeline, platform.node()
            ))
        elif len(possibilities) == 1:
            sif_file = possibilities[0]
            if args.verbose:
                print("The only {} pipeline is version {}".format(
                    args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
                ))
        else:
            sif_file = sorted(possibilities, reverse=True)[0]
            print("Using latest {} pipeline, version {}".format(
                args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
            ))
    else:
        sif_file = Path(os.path.join(
            sif_path, "{}-{}.sif".format(args.pipeline, args.pipeline_version)
        ))
        if os.path.isfile(sif_file):
            print("Found specified image at {}".format(sif_file))
        else:
            print("ERR: No image exists at {}.".format(sif_file))
            sif_file = None

    return sif_file


def context_is_valid(args):
    """ Ensure paths are OK and writable before bothering slurm queues. """

    # Pipeline is available.
    setattr(args, "sif_file", get_singularity_image(args))
    sif_available = args.sif_file is not None

    # Subject is available in rawdata
    if args.rawdata is None:
        print(
            "ERR: 'rawdata' must be provided "
            "in ~/.pipelines.json or as --rawdata."
        )
        sub_available = False
    elif args.subject is None:
        print("ERR: 'subject' must be provided on the command line.")
        sub_available = False
    else:
        sub_available = os.path.exists(
            os.path.join(args.rawdata, "sub-" + args.subject)
        )
        if not sub_available:
            print("ERR: Cannot find a file at '{}'".format(
                os.path.join(args.rawdata, "sub-" + args.subject)
            ))

    # Work directory is writable
    if args.work_directory is None:
        print("There is no working directory.")
        work_writable = False
    else:
        os.makedirs(args.work_directory, exist_ok=True)
        test_w_file = os.path.join(args.work_directory, "test.file")
        with open(test_w_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        work_writable = os.path.exists(test_w_file)
        os.remove(test_w_file)

    # Staging directory is writable (may simply be copied {derivatives})
    if args.staging_directory is None:
        print("There is no staging directory.")
        stage_writable = False
    else:
        os.makedirs(args.staging_directory, exist_ok=True)
        test_s_file = os.path.join(args.staging_directory, "test.file")
        with open(test_s_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        stage_writable = os.path.exists(test_s_file)
        os.remove(test_s_file)

    if args.dry_run and not sif_available:
        print("     Allowing dry-run to continue without singularity image.")
        print("     This batch file will fail if submitted.")
        sif_available = True

    if args.pipeline == "freesurfer":
        retval = (sif_available and sub_available and stage_writable)
    else:
        retval = (sif_available and sub_available and work_writable
                  and stage_writable)

    print("Context is {}valid.".format("" if retval else "NOT "))
    return retval


def main(args):
    """ Entry point """

    if args.verbose:
        print("Asked to process data from 'sub-{}' with options:".format(
            args.subject
        ))
        for arg in vars(args):
            print("  {}: {}".format(arg, getattr(args, arg)))

    if context_is_valid(args):
        batch_script = {
            "freesurfer": FreeSurferBatch,
            "fmriprep": FMRIPrepBatch,
            "mriqc": MRIQCBatch,
            "qsiprep": QSIPrepBatch,
        }[args.pipeline](args)
        batch_script.write()
        batch_script.submit()


if __name__ == "__main__":
    main(get_arguments())
