#!/usr/bin/env python3

# sub_pipeline

import sys
import os
import platform
import pwd
import pathlib
import json
import subprocess

import argparse
from argparse import RawTextHelpFormatter

from datetime import datetime


def retrieve_user_settings(pipeline, verbose=False):
    """ Load user's json and override defaults, but not cmdline """

    # Fill in missing arguments with user settings in home directory
    user_settings_file = os.path.join(
        os.path.expanduser("~"), ".pipelines.json"
    )
    if not os.path.isfile(user_settings_file):
        print("No settings found at {}.".format(user_settings_file))
        return {}

    user_settings = json.load(open(user_settings_file, "r"))
    if verbose:
        print("Found user settings at {}".format(user_settings_file))
    if pipeline not in user_settings:
        if verbose:
            print("  User settings have no pipeline '{}'".format(pipeline))
        return {}

    if verbose:
        print("  Pipeline '{}' found in user settings".format(pipeline))

    return user_settings[pipeline]


def settle_memory(args):
    # Convert memory specs
    if args.memory_mb is None and args.memory_gb is None:
        setattr(args, "memory_mb", 28 * 1024)
        setattr(args, "memory_gb", 28)
    elif args.memory_gb is None:
        setattr(args, "memory_mb", int(args.memory_mb))
        setattr(args, "memory_gb", int(args.memory_mb / 1024))
    elif args.memory_mb is None:
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_gb * 1024))
    else:
        # Both are set, unlikely, but just use what's given.
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_mb))


def settle_threads(args):
    # Convert threads == cpus
    # We only use num_threads, but we'll accept num_cpus as the same thing.
    if args.num_cpus is None and args.num_threads is None:
        setattr(args, "num_cpus", 8)
        setattr(args, "num_threads", 8)
    elif args.num_threads is None:
        setattr(args, "num_cpus", int(args.num_cpus))
        setattr(args, "num_threads", int(args.num_cpus))
    elif args.num_cpus is None:
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_threads))
    else:
        # Both are set, unlikely, but just use what's given.
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_cpus))


def get_arguments():
    """ Parse command line arguments """

    # Start with user settings as defaults
    user_settings = retrieve_user_settings(
        sys.argv[1], verbose=("--verbose" in sys.argv),
    )

    parser = argparse.ArgumentParser(
        description="\n".join([
            "Submit a Singularity pipeline request to SLURM.",
            "",
            "Save consistent settings to ~/.pipelines.json.",
            "Use command-line to override or supply infrequent options.",
            "See README at https://github.com/mfschmidt/jjm_utils/",
        ]),
        formatter_class=RawTextHelpFormatter,
    )
    parser.add_argument(
        "pipeline",
        help="The name of a pipeline to execute",
    )
    parser.add_argument(
        "subject",
        help="The name of a subject to be processed",
    )
    parser.add_argument(
        "--home",
        help="Submitting user's home path",
    )
    parser.add_argument(
        "--userid",
        help="Submitting user's numeric user id",
    )
    parser.add_argument(
        "--username",
        help="Submitting user's user name",
    )
    parser.add_argument(
        "--rawdata",
        default=user_settings.get("rawdata"),
        help="The path to get bids-formatted raw data",
    )
    parser.add_argument(
        "--derivatives",
        default=user_settings.get("derivatives"),
        help="The path to write pipeline derivative output",
    )
    parser.add_argument(
        "--log-file",
        default=user_settings.get("log-file"),
        help="The path to a file for logging details",
    )
    parser.add_argument(
        "--work-directory",
        default=user_settings.get("work-directory"),
        help="The path to read and write temporary files",
    )
    parser.add_argument(
        "--staging-directory",
        default=user_settings.get("staging-directory"),
        help="The path to stage output before copying to derivatives",
    )
    parser.add_argument(
        "--templateflow-directory",
        default=user_settings.get("templateflow-directory"),
        help="The path to read and write cached templateflow data",
    )
    parser.add_argument(
        "--pipeline-version",
        default=user_settings.get("pipeline-version"),
        help="The version of the docker/singularity pipeline to run",
    )
    parser.add_argument(
        "--slurm-partition",
        default=user_settings.get("slurm-partition"),
        help="The slurm partition for this job",
    )
    parser.add_argument(
        "--freesurfer-license",
        default=user_settings.get("freesurfer-license"),
        help="The location of a valid FreeSurfer license file",
    )
    parser.add_argument(
        "--output-resolution",
        default=user_settings.get("output-resolution", "2.0"),
        help="The isotropic resolution in mm for qsiprep output",
    )
    parser.add_argument(
        "--output-space",
        default=user_settings.get("output-space", "T1w"),
        help="Anatomical space for qsiprep output, should stay with T1w.",
    )
    parser.add_argument(
        "--output-spaces",
        default=user_settings.get("output-spaces"),
        help="Anatomical space and resolution for fmiprep output",
    )
    parser.add_argument(
        "--template",
        default=user_settings.get("template", "MNI152NLin2009cAsym"),
        help="The intended template for qsiprep",
    )
    parser.add_argument(
        "--recon-spec",
        default=user_settings.get("recon-spec"),
        help="The pipeline for diffusion, after pre-processing is complete",
    )
    parser.add_argument(
        "--hmc-model",
        default=user_settings.get("hmc-model", "eddy"),
        help="The diffusion correction algorithm, 'eddy' or '3dSHORE'",
    )
    parser.add_argument(
        "--num-cpus",
        default=user_settings.get("num-cpus"),
        help="The number of cpus==threads to allocate for the pipeline.",
    )
    parser.add_argument(
        "--num-threads",
        default=user_settings.get("num-threads"),
        help="The number of threads==cpus to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-mb",
        default=user_settings.get("memory-mb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-gb",
        default=user_settings.get("memory-gb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--stage-output", action="store_true",
        default=user_settings.get("stage-output", False),
        help="Set this flag to cache output before copying to derivatives.",
    )
    parser.add_argument(
        "--tmp-root",
        default=user_settings.get("tmp-root", "/var/tmp"),
        help="Set this path to house default working and staging areas.",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        default=user_settings.get("verbose", False),
        help="set to trigger verbose output",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        default=user_settings.get("dry-run", False),
        help="set to create sbatch script, but NOT submit it.",
    )

    # Determine expected values for variables not provided or overridden
    if "--userid" not in sys.argv:
        sys.argv.append("--userid")
        sys.argv.append(str(os.getuid()))
    if "--username" not in sys.argv:
        sys.argv.append("--username")
        sys.argv.append(pwd.getpwuid(os.getuid())[0])
    if "--home" not in sys.argv:
        sys.argv.append("--home")
        sys.argv.append(os.path.expanduser("~"))

    args = parser.parse_args()
    setattr(args, "timestamp", datetime.now().strftime("%Y%m%d%H%M%S"))

    settle_memory(args)
    settle_threads(args)

    # Generate final defaults, if they aren't provided.
    if getattr(args, "work_directory") is None:
        setattr(args, "work_directory", os.path.join(
            args.tmp_root,
            "_".join([args.pipeline, args.subject, args.timestamp, "working", ])
        ))
    if getattr(args, "templateflow_directory") is None:
        setattr(args, "templateflow_directory", os.path.join(
            args.tmp_root,
            "templateflow"
        ))
    if getattr(args, "slurm_partition") is None:
        setattr(args, "slurm_partition", args.pipeline)

    # If staging output, set up the directory, otherwise just copy derivatives
    if args.stage_output:
        if getattr(args, "staging_directory") is None:
            setattr(args, "staging_directory", os.path.join(
                args.tmp_root,
                "_".join([args.pipeline, args.subject, args.timestamp, "staging", ])
            ))
    else:
        setattr(args, "staging_directory", args.derivatives)

    return args


def get_singularity_image(args):
    """ Return the path to a singularity file, if it exists, else None """

    sif_path = "/var/lib/singularity/images"
    sif_file = None
    possibilities = []

    if args.pipeline_version is None:
        if args.verbose:
            print("No version specified for {} pipeline.".format(
                args.pipeline
            ))
        possibilities = list(pathlib.Path(sif_path).glob(
            "{}-*.sif".format(args.pipeline)
        ))
        if len(possibilities) <= 0:
            print("ERR: No image exists for {} pipeline on {}.".format(
                args.pipeline, platform.node()
            ))
        elif len(possibilities) == 1:
            sif_file = possibilities[0]
            if args.verbose:
                print("The only {} pipeline is version {}".format(
                    args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
                ))
        else:
            sif_file = sorted(possibilities, reverse=True)[0]
            print("Using latest {} pipeline, version {}".format(
                args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
            ))
    else:
        sif_file = pathlib.Path(os.path.join(
            sif_path, "{}-{}.sif".format(args.pipeline, args.pipeline_version)
        ))
        if not os.path.isfile(sif_file):
            print("ERR: No image exists at {}.".format(sif_file))
            sif_file = None

    return sif_file


def context_is_valid(args):
    """ Ensure paths are OK and writable before bothering slurm queues. """

    # Pipeline is available.
    setattr(args, "sif_file", get_singularity_image(args))
    sif_available = args.sif_file is not None

    # Subject is available in rawdata
    if args.rawdata is None:
        print(
            "ERR: 'rawdata' must be provided "
            "in ~/.pipelines.json or as --rawdata."
        )
        sub_available = False
    elif args.subject is None:
        print("ERR: 'subject' must be provided on the command line.")
        sub_available = False
    else:
        sub_available = os.path.exists(
            os.path.join(args.rawdata, "sub-" + args.subject)
        )
        if not sub_available:
            print("ERR: Cannot find a file at '{}'".format(
                os.path.join(args.rawdata, "sub-" + args.subject)
            ))

    # Work directory is writable
    if args.work_directory is None:
        work_writable = False
    else:
        os.makedirs(args.work_directory, exist_ok=True)
        test_w_file = os.path.join(args.work_directory, "test.file")
        with open(test_w_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        work_writable = os.path.exists(test_w_file)
        os.remove(test_w_file)

    # Staging directory is writable (may simply be copied {derivatives})
    if args.staging_directory is None:
        stage_writable = False
    else:
        os.makedirs(args.staging_directory, exist_ok=True)
        test_s_file = os.path.join(args.staging_directory, "test.file")
        with open(test_s_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        stage_writable = os.path.exists(test_s_file)
        os.remove(test_s_file)

    if args.dry_run:
        print("     Allowing dry-run to continue without singularity image.")
        print("     This batch file will fail if submitted.")
        sif_available = True

    retval = (sif_available and sub_available and
              work_writable and stage_writable)
    print("Context is {}valid.".format("" if retval else "NOT "))
    return retval


def write_batch_script(args):
    """ Create the batch script to batch the pipeline execution. """

    os.makedirs(os.path.join(args.home, "bin", "slurm"), exist_ok=True)
    script_name = "_".join([
        "batch", "sub-" + args.subject, args.pipeline, args.timestamp,
    ]) + ".sbatch"
    batch_file_path = pathlib.Path(args.home, "bin", "slurm", script_name)
    if args.log_file is None:
        setattr(args, "log_file", str(batch_file_path).replace(
            ".sbatch", ".%j.out"
        ))
    with open(batch_file_path, "w") as batch_file:
        tf_env = "SINGULARITYENV_TEMPLATEFLOW_HOME"
        batch_file.write(
            f"#!/bin/bash\n"
            f"#SBATCH --job-name={args.subject}\n"
            f"#SBATCH --partition={args.slurm_partition}\n"
            f"#SBATCH --output={args.log_file}\n"
            f"#SBATCH --error={args.log_file}\n"
            f"#SBATCH --time=0\n"
            f"#SBATCH --ntasks-per-node={args.num_threads}\n"
            f"#SBATCH --mem={args.memory_mb}\n"
            f"#SBATCH --chdir={args.tmp_root}\n"
            f"#SBATCH --export={tf_env}=/opt/templateflow\n"
            f"\n"
            f"mkdir -p {args.work_directory}\n"
            f"mkdir -p {args.templateflow_directory}\n"
            f"mkdir -p {args.staging_directory}\n"
            f"\n"
            f"singularity run --cleanenv \\\n"
            f"  -B {args.rawdata}:/rawdata:ro \\\n"
            f"  -B {args.staging_directory}:/out \\\n"  # may be {derivatives}
            f"  -B {args.work_directory}:/work \\\n"
            f"  -B {args.templateflow_directory}:/opt/templateflow \\\n"
            f"  -B {args.freesurfer_license}:/opt/freesurfer/license.txt \\\n"
            f"  {args.sif_file} \\\n"
            f"  /rawdata /out participant \\\n"
            f"  -w /work \\\n"
            f"  --participant-label {args.subject} \\\n"
        )
        # TODO: Somewhere in here, pass along any arguments we did not
        #       explicitly handle.
        if args.pipeline == "fmriprep":
            batch_file.write(
                f"  --fs-license-file /opt/freesurfer/license.txt \\\n"
            )
            if args.pipeline_version.startswith("1."):
                if args.output_space is not None:
                    batch_file.write(
                        f"  --nthreads {args.num_threads} --mem_mb {args.memory_mb} \\\n"
                        f"  --output-space {args.output_space} \\\n"
                        f"  --template {args.template} \\\n"
                        f"  --template-resampling-grid {args.output_resolution}mm\n"
                    )
            else:
                if args.output_spaces is not None:
                    batch_file.write(
                        f"  --nprocs {args.num_threads} --mem {args.memory_gb} \\\n"
                        f"  --output-spaces {args.output_spaces}\n"
                    )
        elif args.pipeline == "qsiprep":
            batch_file.write(
                f"  --fs-license-file /opt/freesurfer/license.txt \\\n"
                f"  --output-resolution {args.output_resolution} \\\n"
                f"  --hmc-model {args.hmc_model} \\\n"
                f"  --output-space {args.output_space} \\\n"
                f"  --template {args.template} \\\n"
                f"  --nthreads {args.num_threads} --mem_mb {args.memory_mb}\n"
            )
            if (args.recon_spec is not None) and (args.recon_spec != "none"):
                batch_file.write(f"  --recon-spec {args.recon_spec} \\\n")
        else:  # mriqc
            batch_file.write(
                f"  --nprocs {args.num_threads} --mem_gb {args.memory_gb}\n"
            )
        batch_file.write(
            f"\nif [[ \"$?\" != \"0\" ]]; then\n"
            f"  echo \"Container run failed. Avoiding clean-up.\"\n"
            f"  echo \"$(hostname):{args.work_directory}\"\n"
            f"  echo \"$(hostname):{args.staging_directory}\"\n"
            f"  exit $?\n"
            f"fi\n\n"
        )
        if args.stage_output:
            batch_file.write(
                "# Format and copy output to final destination.\n"
            )
            batch_file.write("chown {}:{} --recursive {}\n".format(
                args.username, args.userid, args.staging_directory
            ))
            batch_file.write("rsync -ah {}/{}/sub-{}* {}/{}/\n".format(
                args.staging_directory, args.pipeline, args.subject,
                args.derivatives, args.pipeline
            ))
            if args.pipeline == "fmriprep":
                sfs_dir = f"{args.derivatives}/freesurfer/sub-{args.subject}"
                batch_file.write(
                    f"# Find an open location for freesurfer output.\n"
                    f"F={sfs_dir}\n"
                    f"if [[ -e ${{F}} ]]; then\n"
                    f"  N=0\n"
                    f"  while [[ -e ${{F}} ]]; do\n"
                    f"    echo \"will not overwrite existing data, ${{F}}\"\n"
                    f"    N=$(( N + 1 ))\n"
                    f"    F=${{F%%.*}}.${{N}}\n"
                    f"  done\n"
                    f"fi\n"
                    f"rsync -ah ${{F}}* {args.derivatives}/freesurfer/\n"
                )
            elif args.pipeline == "qsiprep":
                batch_file.write(
                    "rsync -ah {}/qsirecon/sub-{}* {}/qsirecon/\n".format(
                        args.staging_directory, args.subject, args.derivatives,
                    )
                )
            batch_file.write(f"\nrm -rf {args.staging_directory}\n")
        batch_file.write(
            f"\necho \"Job complete at $(date)\"\n"
            f"rm -rf {args.work_directory}\n"
        )
    return batch_file_path


def submit_batch_script(sbatch_file, dry_run=False, verbose=False):
    """ Create the batch script to batch the pipeline execution. """
    if verbose:
        print("|==========---------- Batch file follows ----------==========|")
        subprocess.run(["cat", str(sbatch_file), ], check=True)
        print("|==========----------   end batch file   ----------==========|")
    print("Batch file was saved to {}".format(str(sbatch_file)))
    if dry_run:
        print("NOT submitting job because dry-run is True.")
    else:
        print("Submitting job at {}".format(
            datetime.now().strftime("%Y%m%d %H:%M:%S")
        ))
        sbatch_submit_process = subprocess.run(
            ["sbatch", str(sbatch_file), ], check=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
        )
        print(sbatch_submit_process.stdout.decode("utf-8"))


def main(args):
    """ Entry point """

    if args.verbose:
        print("Asked to process data from 'sub-{}' with options:".format(
            args.subject
        ))
        for arg in vars(args):
            print("  {}: {}".format(arg, getattr(args, arg)))

    if context_is_valid(args):
        batch_script = write_batch_script(args)
        submit_batch_script(
            batch_script, dry_run=args.dry_run, verbose=args.verbose
        )


if __name__ == "__main__":
    main(get_arguments())
