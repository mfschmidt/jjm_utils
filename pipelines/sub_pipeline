#!/usr/bin/env python3

# sub_pipeline

import sys
import os
import platform
import pwd
import pathlib
import json
import subprocess

import argparse
from argparse import RawTextHelpFormatter

from datetime import datetime


def retrieve_user_settings(pipeline, verbose=False):
    """ Load user's json and override defaults, but not cmdline """

    # Fill in missing arguments with user settings in home directory
    user_settings_file = os.path.join(
        os.path.expanduser("~"), ".pipelines.json"
    )
    if not os.path.isfile(user_settings_file):
        print("No settings found at {}.".format(user_settings_file))
        return {}

    user_settings = json.load(open(user_settings_file, "r"))
    if verbose:
        print("Found user settings at {}".format(user_settings_file))
    if pipeline not in user_settings:
        if verbose:
            print("  User settings have no pipeline '{}'".format(pipeline))
        return {}

    if verbose:
        print("  Pipeline '{}' found in user settings".format(pipeline))

    return user_settings[pipeline]


def settle_memory(args):
    """ Convert memory specs.
        Different pipelines want different keys, make all available.
    """
    if args.memory_mb is None and args.memory_gb is None:
        # Default memory settings
        # FreeSurfer docs request 8GB, this should be overkill.
        # fMRIPrep docs request 8GB, this should be overkill.
        setattr(args, "memory_mb", 16 * 1024)
        setattr(args, "memory_gb", 16)
    elif args.memory_gb is None:
        setattr(args, "memory_mb", int(args.memory_mb))
        setattr(args, "memory_gb", int(args.memory_mb / 1024))
    elif args.memory_mb is None:
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_gb * 1024))
    else:
        # Both are set. -- unlikely, but fine
        setattr(args, "memory_gb", int(args.memory_gb))
        setattr(args, "memory_mb", int(args.memory_mb))


def settle_threads(args):
    """ Convert threads == cpus
        We only use num_threads, but we'll accept num_cpus as the same thing.
    """
    if args.num_cpus is None and args.num_threads is None:
        setattr(args, "num_cpus", 8)
        setattr(args, "num_threads", 8)
    elif args.num_threads is None:
        setattr(args, "num_cpus", int(args.num_cpus))
        setattr(args, "num_threads", int(args.num_cpus))
    elif args.num_cpus is None:
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_threads))
    else:
        # Both are set, unlikely, but just use what's given.
        setattr(args, "num_threads", int(args.num_threads))
        setattr(args, "num_cpus", int(args.num_cpus))


def settle_subject(args):
    """ Allow "U00000" or "sub-U00000" as the same thing. """
    if args.subject.startswith("sub-"):
        setattr(args, "subject", args.subject[4:])


def get_arguments():
    """ Parse command line arguments """

    # Start with user settings as defaults
    user_settings = retrieve_user_settings(
        sys.argv[1], verbose=("--verbose" in sys.argv),
    )

    parser = argparse.ArgumentParser(
        description="\n".join([
            "Submit a Singularity pipeline request to SLURM.",
            "",
            "Save consistent settings to ~/.pipelines.json.",
            "Use command-line to override or supply infrequent options.",
            "See README at https://github.com/mfschmidt/jjm_utils/",
        ]),
        formatter_class=RawTextHelpFormatter,
    )
    parser.add_argument(
        "pipeline",
        help="The name of a pipeline to execute",
    )
    parser.add_argument(
        "subject",
        help="The name of a subject to be processed",
    )
    parser.add_argument(
        "--home",
        help="Submitting user's home path",
    )
    parser.add_argument(
        "--userid",
        help="Submitting user's numeric user id",
    )
    parser.add_argument(
        "--username",
        help="Submitting user's user name",
    )
    parser.add_argument(
        "--rawdata",
        default=user_settings.get("rawdata"),
        help="The path to get bids-formatted raw data",
    )
    parser.add_argument(
        "--derivatives",
        default=user_settings.get("derivatives"),
        help="The path to write pipeline derivative output",
    )
    parser.add_argument(
        "--subjects-dir",
        default=user_settings.get("subjects-dir"),
        help="The path to write freesurfer output",
    )
    parser.add_argument(
        "--log-file",
        default=user_settings.get("log-file"),
        help="The path to a file for logging details",
    )
    parser.add_argument(
        "--work-directory",
        default=user_settings.get("work-directory"),
        help="The path to read and write temporary files",
    )
    parser.add_argument(
        "--staging-directory",
        default=user_settings.get("staging-directory"),
        help="The path to stage output before copying to derivatives",
    )
    parser.add_argument(
        "--templateflow-directory",
        default=user_settings.get("templateflow-directory"),
        help="The path to read and write cached templateflow data",
    )
    parser.add_argument(
        "--pipeline-version",
        default=user_settings.get("pipeline-version"),
        help="The version of the docker/singularity pipeline to run",
    )
    parser.add_argument(
        "--slurm-partition",
        default=user_settings.get("slurm-partition"),
        help="The slurm partition for this job",
    )
    parser.add_argument(
        "--freesurfer-license",
        default=user_settings.get("freesurfer-license"),
        help="The location of a valid FreeSurfer license file",
    )
    parser.add_argument(
        "--output-resolution",
        default=user_settings.get("output-resolution", "2.0"),
        help="The isotropic resolution in mm for qsiprep output",
    )
    parser.add_argument(
        "--template-resampling-grid",
        default=user_settings.get("template-resampling-grid", "2mm"),
        help="The isotropic resolution for old fmriprep output",
    )
    parser.add_argument(
        "--output-space",
        default=user_settings.get("output-space", "T1w"),
        help="Anatomical space for qsiprep and old fmriprep output,"
             " should stay with T1w.",
    )
    parser.add_argument(
        "--output-spaces",
        default=user_settings.get("output-spaces"),
        help="Anatomical space and resolution for new fmiprep output",
    )
    parser.add_argument(
        "--template",
        default=user_settings.get("template", "MNI152NLin2009cAsym"),
        help="The intended template for qsiprep and old fmriprep",
    )
    parser.add_argument(
        "--recon-spec",
        default=user_settings.get("recon-spec"),
        help="The pipeline for diffusion, after pre-processing is complete",
    )
    parser.add_argument(
        "--hmc-model",
        default=user_settings.get("hmc-model", "eddy"),
        help="The diffusion correction algorithm, 'eddy' or '3dSHORE'",
    )
    parser.add_argument(
        "--input-t1",
        default=user_settings.get("input-t1", ""),
        help="Specify a specific T1w image for FreeSurfer input.",
    )
    parser.add_argument(
        "--input-t2",
        default=user_settings.get("input-t2", ""),
        help="Specify a specific T2w image for FreeSurfer input.",
    )
    parser.add_argument(
        "--num-cpus",
        default=user_settings.get("num-cpus"),
        help="The number of cpus==threads to allocate for the pipeline.",
    )
    parser.add_argument(
        "--num-threads",
        default=user_settings.get("num-threads"),
        help="The number of threads==cpus to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-mb",
        default=user_settings.get("memory-mb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--memory-gb",
        default=user_settings.get("memory-gb"),
        help="The amount of memory to allocate for the pipeline.",
    )
    parser.add_argument(
        "--cm", action="store_true",
        default=user_settings.get("cm", False),
        help="Set this flag to use sub-mm voxels in FreeSurfer.",
    )
    parser.add_argument(
        "--stage-output", action="store_true",
        default=user_settings.get("stage-output", False),
        help="Set this flag to cache output before copying to derivatives.",
    )
    parser.add_argument(
        "--tmp-root",
        default=user_settings.get("tmp-root", "/var/tmp"),
        help="Set this path to house default working and staging areas.",
    )
    parser.add_argument(
        "--verbose", action="store_true",
        default=user_settings.get("verbose", False),
        help="set to trigger verbose output",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        default=user_settings.get("dry-run", False),
        help="set to create sbatch script, but NOT submit it.",
    )

    # Determine expected values for variables not provided or overridden
    if "--userid" not in sys.argv:
        sys.argv.append("--userid")
        sys.argv.append(str(os.getuid()))
    if "--username" not in sys.argv:
        sys.argv.append("--username")
        sys.argv.append(pwd.getpwuid(os.getuid())[0])
    if "--home" not in sys.argv:
        sys.argv.append("--home")
        sys.argv.append(os.path.expanduser("~"))

    args = parser.parse_args()
    setattr(args, "timestamp", datetime.now().strftime("%Y%m%d%H%M%S"))

    settle_memory(args)
    settle_threads(args)
    settle_subject(args)

    # Generate final defaults, if they aren't provided.
    if getattr(args, "work_directory") is None:
        setattr(args, "work_directory", os.path.join(
            args.tmp_root,
            "_".join([
                args.pipeline, args.subject, args.timestamp, "working",
            ])
        ))
    if getattr(args, "templateflow_directory") is None:
        setattr(args, "templateflow_directory", os.path.join(
            args.tmp_root, "templateflow"
        ))
    if getattr(args, "slurm_partition") is None:
        setattr(args, "slurm_partition", args.pipeline)

    # If staging output, set up the directory, otherwise just copy derivatives
    if args.stage_output:
        if getattr(args, "staging_directory") is None:
            setattr(args, "staging_directory", os.path.join(
                args.tmp_root,
                "_".join([
                    args.pipeline, args.subject, args.timestamp, "staging",
                ])
            ))
    else:
        # If we are writing output directly, set staging_directory there.
        if args.pipeline == "freesurfer":
            setattr(args, "staging_directory", args.subjects_dir)
        else:
            setattr(args, "staging_directory", args.derivatives)

    return args


def get_singularity_image(args):
    """ Return the path to a singularity file, if it exists, else None """

    sif_path = "/var/lib/singularity/images"
    sif_file = None
    possibilities = []

    if args.pipeline_version is None:
        if args.verbose:
            print("No version specified for {} pipeline.".format(
                args.pipeline
            ))
        possibilities = [str(p) for p in pathlib.Path(sif_path).glob(
            "{}-*.sif".format(args.pipeline)
        )]
        if len(possibilities) <= 0:
            print("ERR: No image exists for {} pipeline on {}.".format(
                args.pipeline, platform.node()
            ))
        elif len(possibilities) == 1:
            sif_file = possibilities[0]
            if args.verbose:
                print("The only {} pipeline is version {}".format(
                    args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
                ))
        else:
            sif_file = sorted(possibilities, reverse=True)[0]
            print("Using latest {} pipeline, version {}".format(
                args.pipeline, sif_file[sif_file.rfind("-") + 1:-4]
            ))
    else:
        sif_file = pathlib.Path(os.path.join(
            sif_path, "{}-{}.sif".format(args.pipeline, args.pipeline_version)
        ))
        if os.path.isfile(sif_file):
            print("Found specified image at {}".format(sif_file))
        else:
            print("ERR: No image exists at {}.".format(sif_file))
            sif_file = None

    return sif_file


def context_is_valid(args):
    """ Ensure paths are OK and writable before bothering slurm queues. """

    # Pipeline is available.
    setattr(args, "sif_file", get_singularity_image(args))
    sif_available = args.sif_file is not None

    # Subject is available in rawdata
    if args.rawdata is None:
        print(
            "ERR: 'rawdata' must be provided "
            "in ~/.pipelines.json or as --rawdata."
        )
        sub_available = False
    elif args.subject is None:
        print("ERR: 'subject' must be provided on the command line.")
        sub_available = False
    else:
        sub_available = os.path.exists(
            os.path.join(args.rawdata, "sub-" + args.subject)
        )
        if not sub_available:
            print("ERR: Cannot find a file at '{}'".format(
                os.path.join(args.rawdata, "sub-" + args.subject)
            ))

    # Work directory is writable
    if args.work_directory is None:
        print("There is no working directory.")
        work_writable = False
    else:
        os.makedirs(args.work_directory, exist_ok=True)
        test_w_file = os.path.join(args.work_directory, "test.file")
        with open(test_w_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        work_writable = os.path.exists(test_w_file)
        os.remove(test_w_file)

    # Staging directory is writable (may simply be copied {derivatives})
    if args.staging_directory is None:
        print("There is no staging directory.")
        stage_writable = False
    else:
        os.makedirs(args.staging_directory, exist_ok=True)
        test_s_file = os.path.join(args.staging_directory, "test.file")
        with open(test_s_file, "w") as test_file:
            test_file.write("Just a test, this file can be deleted.")
        stage_writable = os.path.exists(test_s_file)
        os.remove(test_s_file)

    if args.dry_run and not sif_available:
        print("     Allowing dry-run to continue without singularity image.")
        print("     This batch file will fail if submitted.")
        sif_available = True

    if args.pipeline == "freesurfer":
        retval = (sif_available and sub_available and stage_writable)
    else:
        retval = (sif_available and sub_available and work_writable
                  and stage_writable)

    print("Context is {}valid.".format("" if retval else "NOT "))
    return retval


def get_batch_header(args):
    """ Return the header with SLURM variables to the sbatch file.
    """
    tf_env = "SINGULARITYENV_TEMPLATEFLOW_HOME"
    return "\n".join([
        "#!/bin/bash",
        "",
        f"#SBATCH --job-name={args.subject}",
        f"#SBATCH --partition={args.slurm_partition}",
        f"#SBATCH --output={args.log_file}",
        f"#SBATCH --error={args.log_file}",
        "#SBATCH --time=0",
        f"#SBATCH --ntasks-per-node={args.num_threads}",
        f"#SBATCH --mem={args.memory_mb}",
        f"#SBATCH --chdir={args.tmp_root}",
        f"#SBATCH --export={tf_env}={args.tmp_root}/templateflow",
        "",
        f"mkdir -p {args.work_directory}",
        f"mkdir -p {args.templateflow_directory}",
        f"mkdir -p {args.staging_directory}",
        "",
        "",
    ])


def get_singularity_command(args):
    """ Return the singularity command to execute in the sbatch file.

        Some of these mounts don't matter for all situations, but mounting
        them doesn't hurt either, so they all stay.
    """
    base_command = [
        "singularity run --cleanenv",
        f"  -B {args.rawdata}:/rawdata:ro",
        f"  -B {args.staging_directory}:/out",  # may be {derivatives}
    ]
    if args.pipeline != "freesurfer":
        base_command += [
            f"  -B {args.work_directory}:/work",
            f"  -B {args.templateflow_directory}:/opt/templateflow",
            f"  -B {args.freesurfer_license}:/opt/freesurfer/license.txt",
            "  -B /data:/data:ro",
        ]
    base_command += [
        f"  {args.sif_file}",
    ]
    return " \\\n".join(base_command)


def get_mriqc_singularity_arguments(args):
    """ Return the arguments necessary for mriqc, specifically
    """
    return " \\\n".join([
        "  /rawdata /out participant",
        f"  -w /work --participant-label {args.subject}",
        f"  --nprocs {args.num_threads}",
        f"  --mem_gb {args.memory_gb}",
    ])


def get_fmriprep_singularity_arguments(args):
    """ Return the arguments necessary for fmriprep, specifically
    """
    further_arguments = [
        "  /rawdata /out participant",
        f"  -w /work --participant-label {args.subject}",
        "  --fs-license-file /opt/freesurfer/license.txt",
    ]
    if args.pipeline_version.startswith("1."):
        if args.output_space is not None:
            further_arguments += [
                f"  --nthreads {args.num_threads}",
                f"  --mem_mb {args.memory_mb}",
                f"  --output-space {args.output_space}",
                f"  --template {args.template}",
                f"  --template-resampling-grid {args.template_resampling_grid}",
            ]
    else:
        if args.output_spaces is not None:
            further_arguments += [
                f"  --nprocs {args.num_threads} --mem {args.memory_gb}",
                f"  --output-spaces {args.output_spaces}",
            ]
    return " \\\n".join(further_arguments)


def get_qsiprep_singularity_arguments(args):
    """ Return the arguments necessary for fmriprep, specifically.
    """
    further_arguments = [
        "  /rawdata /out participant",
        f"  -w /work --participant-label {args.subject}",
        "  --fs-license-file /opt/freesurfer/license.txt",
        f"  --output-resolution {args.output_resolution}",
        f"  --hmc-model {args.hmc_model}",
        f"  --output-space {args.output_space}",
        f"  --template {args.template}",
        f"  --nthreads {args.num_threads}",
        f"  --mem_mb {args.memory_mb}",
    ]
    if (args.recon_spec is not None) and (args.recon_spec != "none"):
        further_arguments.append(
            f"  --recon-spec {args.recon_spec}"
        )
    return " \\\n".join(further_arguments)


def get_freesurfer_singularity_arguments(args, stage="recon"):
    """ Return the arguments necessary for freesurfer, specifically.
    """

    # Use specified T1w & T2w files, or find them in BIDS-compatible raw data.
    subject_path = pathlib.Path(args.rawdata) / f"sub-{args.subject}"
    if args.input_t1 == "":
        t1s = [str(p) for p in subject_path.glob("**/*T1w.nii.gz")]
    else:
        t1s = [args.input_t1, ]
    if args.input_t2 == "":
        t2s = [str(p) for p in subject_path.glob("**/*T2w.nii.gz")]
    else:
        t2s = [args.input_t2, ]

    # Use -cm if sub-1mm voxels are desired.
    cm_str = " -cm" if args.cm else ""

    if stage == "recon":
        further_arguments = [
            f"  recon-all -s sub-{args.subject}",
        ] + [
            f"  -i {t1.replace(args.rawdata, '/rawdata')}" for t1 in t1s
        ] + [
            f"  -T2 {t2.replace(args.rawdata, '/rawdata')}" for t2 in t2s
        ] + [
            f"  -sd /out -openmp {args.num_cpus}{cm_str} -all",
        ]
    elif stage == "hippocampus_t1":
        further_arguments = [f"  segmentHA_T1.sh sub-{args.subject} /out", ]
    elif stage == "hippocampus_t2":
        if len(t2s) > 0:
            inner_t2 = t2s[0].replace(args.rawdata, '/rawdata')
            further_arguments = [
                f"  segmentHA_T2.sh sub-{args.subject} {inner_t2} T2 1 /out",
            ]

    elif stage == "brainstem":
        further_arguments = [f"  segmentBS.sh sub-{args.subject} /out", ]
    return " \\\n".join(further_arguments)


def get_freesurfer_addons(args):
    """ Return the text of additional processes for FreeSurfer 7 runs.
    """

    addons = [
        get_singularity_command(args) + " \\",
        get_freesurfer_singularity_arguments(args, "hippocampus_t1"),
        failure_text("Hippocampal Subfields 1st Segmentation", args),
    ]
    subject_path = pathlib.Path(args.rawdata) / f"sub-{args.subject}"
    if args.input_t2 == "":
        t2s = [str(p) for p in subject_path.glob("**/*T2w.nii.gz")]
    else:
        t2s = [args.input_t2, ]
    if len(t2s) > 0:
        addons += [
            get_singularity_command(args) + " \\",
            get_freesurfer_singularity_arguments(args, "hippocampus_t2"),
            failure_text("Hippocampal Subfields 2nd Segmentation", args),
        ]
    addons += [
        get_singularity_command(args) + " \\",
        get_freesurfer_singularity_arguments(args, "brainstem"),
        failure_text("Brainstem Nuclei Segmentation", args),
    ]
    return "\n".join(addons)


def failure_text(what_failed, args):
    """ Return the text to report a failure in the sbatch file.
    """

    failure_lines = [
        "\nif [[ \"$?\" != \"0\" ]]; then",
        f"  echo \"{what_failed} failed.\"",
        f"  echo \"$(hostname):{args.work_directory}\"",
        f"  echo \"$(hostname):{args.staging_directory}\"",
        "  exit $?",
        "fi\n",
    ]
    return "\n".join(failure_lines) + "\n"


def write_batch_script(args):
    """ Create the batch script to batch the pipeline execution. """

    # Create the path and figure out the file names.
    os.makedirs(os.path.join(args.home, "bin", "slurm"), exist_ok=True)
    script_name = "_".join([
        "batch", "sub-" + args.subject, args.pipeline, args.timestamp,
    ]) + ".sbatch"
    batch_file_path = pathlib.Path(args.home, "bin", "slurm", script_name)
    if args.log_file is None:
        setattr(args, "log_file", str(batch_file_path).replace(
            ".sbatch", ".%j.out"
        ))

    # Open and write the sbatch file.
    with open(batch_file_path, "w") as batch_file:
        batch_file.write(get_batch_header(args))
        batch_file.write(get_singularity_command(args) + " \\\n")
        # TODO: Somewhere in here, pass along any arguments we did not
        #       explicitly handle.
        if args.pipeline == "fmriprep":
            batch_file.write(get_fmriprep_singularity_arguments(args))
        elif args.pipeline == "qsiprep":
            batch_file.write(get_qsiprep_singularity_arguments(args))
        elif args.pipeline == "freesurfer":
            batch_file.write(get_freesurfer_singularity_arguments(args))
        elif args.pipeline == "mriqc":
            batch_file.write(get_mriqc_singularity_arguments(args))
        batch_file.write("\n")
        batch_file.write(failure_text("Container", args))

        # FreeSurfer does additional segmentations as separate processes
        if args.pipeline == "freesurfer":
            # Write commands to execute sub-segmentations
            batch_file.write(get_freesurfer_addons(args))

        # If we are staging output, handle moving it to its final home
        if args.stage_output:
            batch_file.write(
                "# Format and copy output to final destination.\n"
            )
            batch_file.write("chown {}:{} --recursive {}\n".format(
                args.username, args.userid, args.staging_directory
            ))
            batch_file.write("rsync -ah {}/{}/sub-{}* {}/{}/\n".format(
                args.staging_directory, args.pipeline, args.subject,
                args.derivatives, args.pipeline
            ))
            if args.pipeline == "fmriprep":
                sfs_dir = f"{args.derivatives}/freesurfer/sub-{args.subject}"
                batch_file.write(
                    f"# Find an open location for freesurfer output.\n"
                    f"F={sfs_dir}\n"
                    f"if [[ -e ${{F}} ]]; then\n"
                    f"  N=0\n"
                    f"  while [[ -e ${{F}} ]]; do\n"
                    f"    echo \"will not overwrite existing data, ${{F}}\"\n"
                    f"    N=$(( N + 1 ))\n"
                    f"    F=${{F%%.*}}.${{N}}\n"
                    f"  done\n"
                    f"fi\n"
                    f"rsync -ah ${{F}}* {args.derivatives}/freesurfer/\n"
                )
            elif args.pipeline == "qsiprep":
                batch_file.write(
                    "rsync -ah {}/qsirecon/sub-{}* {}/qsirecon/\n".format(
                        args.staging_directory, args.subject, args.derivatives,
                    )
                )
            elif args.pipeline == "freesurfer":
                batch_file.write(
                    "rsync -ah {}/sub-{} {}/\n".format(
                        args.staging_directory, args.subject,
                        args.subjects_dir,
                    )
                )
        batch_file.write(
            f"echo \"Job complete at $(date)\"\n"
            f"rm -rf {args.work_directory}\n"
            f"# leave the templateflow directory for next time.\n"
        )
        if args.stage_output:
            batch_file.write(
                f"rm -rf {args.staging_directory}\n"
            )
    return batch_file_path


def submit_batch_script(sbatch_file, dry_run=False, verbose=False):
    """ Create the batch script to batch the pipeline execution. """
    if verbose:
        print("|==========---------- Batch file follows ----------==========|")
        subprocess.run(["cat", str(sbatch_file), ], check=True)
        print("|==========----------   end batch file   ----------==========|")
    print("Batch file was saved to {}".format(str(sbatch_file)))
    if dry_run:
        print("NOT submitting job because dry-run is True.")
    else:
        print("Submitting job at {}".format(
            datetime.now().strftime("%Y%m%d %H:%M:%S")
        ))
        sbatch_submit_process = subprocess.run(
            ["sbatch", str(sbatch_file), ], check=True,
            stdout=subprocess.PIPE, stderr=subprocess.STDOUT,
        )
        print(sbatch_submit_process.stdout.decode("utf-8"))


def main(args):
    """ Entry point """

    if args.verbose:
        print("Asked to process data from 'sub-{}' with options:".format(
            args.subject
        ))
        for arg in vars(args):
            print("  {}: {}".format(arg, getattr(args, arg)))

    if context_is_valid(args):
        batch_script = write_batch_script(args)
        submit_batch_script(
            batch_script, dry_run=args.dry_run, verbose=args.verbose
        )


if __name__ == "__main__":
    main(get_arguments())
